import re,requests
import codecs
import jieba
import jieba.analyse
import numpy as np
from string import digits, ascii_letters

#fr1 = 'C:/Users/Administrator/Desktop/2.txt'
#fr2 = 'C:/Users/Administrator/Desktop/4.txt'

class simhash:
    def __init__(self,content):
        self.simhash=self.simhash(content)

    def __str__(self):
        return str(self.simhash)

    def simhash(self,content):
        #seg = jieba.cut(content)
        #jieba.analyse.set_stop_words('stopword.txt')
        keyWord = jieba.analyse.extract_tags(
            '|'.join(content), topK=10, withWeight=True, allowPOS=())#在这里对jieba的tfidf.py进行了修改
        #将tags = sorted(freq.items(), key=itemgetter(1), reverse=True)修改成tags = sorted(freq.items(), key=itemgetter(1,0), reverse=True)
        #即先按照权重排序，再按照词排序
        keyList = []
        print(keyWord)
        for feature, weight in keyWord:
            weight = int(weight * 10)
            feature = self.string_hash(feature)
            temp = []
            for i in feature:
                if(i == '1'):
                    temp.append(weight)
                else:
                    temp.append(-weight)
            # print(temp)
            keyList.append(temp)
        list1 = np.sum(np.array(keyList), axis=0)
        #print(list1)
        if(keyList==[]): #编码读不出来
            return '00'
        simhash = ''
        for i in list1:
            if(i > 0):
                simhash = simhash + '1'
            else:
                simhash = simhash + '0'
        return simhash

    def similarity(self, other):
        a = float(self.simhash)
        b = float(other.simhash)
        if a > b : return b / a
        else: return a / b

    def string_hash(self,source):
        if source == "":
            return 0
        else:
            x = ord(source[0]) << 7
            m = 1000003
            mask = 2 ** 128 - 1
            for c in source:
                x = ((x * m) ^ ord(c)) & mask
            x ^= len(source)
            if x == -1:
                x = -2
            x = bin(x).replace('0b', '').zfill(64)[-64:]
            #print(source,x)

            return str(x)

    def hammingDis(self, com):
        t1 = '0b' + self.simhash
        t2 = '0b' + com.simhash
        n = int(t1, 2) ^ int(t2, 2)
        i = 0
        while n:
            n &= (n - 1)
            i += 1
        return i


def get_line():
    punc = './ <>_ - - = ", 。，？！“”：‘’@#￥% … &×（）——+【】{};；● &～| \s:'
    stoplist = {}.fromkeys([line.rstrip() for line in
                           codecs.open(r"C:/Users/Administrator/Desktop/word_level/data/中文停用词库.txt", 'r', 'gbk')])

    url = 'http://172.10.11.235:7099/circle/index/titleSearch?keyword=6IyD5Yaw5Yaw'
    r = requests.get(url)
    response = r.json()
    dicts = response['Data']
    dict1 = dicts[15]
    for key in sorted(dict1.keys()):
        content1 = dict1['introduce'] #introduce
    # print(content1)
    dict2 = dicts[4]
    for key in sorted(dict2.keys()):
        content2 = dict2['introduce'] #articleTitle
    # print(content2)


    a = re.sub(r'alt=.+?inline=', '', content1)
    text1 = re.sub(r'[^\w]+', '', a)
    line1 = re.sub(r"[{}]+".format(punc + ascii_letters + digits), "", text1)


    b = re.sub(r'alt=.+?inline=', '', content2)
    text2 = re.sub(r'[^\w]+', '', b)
    line2 = re.sub(r"[{}]+".format(punc + ascii_letters + digits), "", text2)
    hash1 = simhash(line1.split())
    hash2 = simhash(line2.split())
    print(hash1.hammingDis(hash2))
    if hash1.hammingDis(hash2) <= 8:
        print('文本相似')
    else:
        print('文本不相似')
    print(line1)
    print(line2)


if __name__ == '__main__':
    get_line()
