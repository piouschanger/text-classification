import re
import codecs
import jieba
import jieba.analyse
import numpy as np

add_punc = './ <>_ - - = ", 。，？！“”：‘’@#￥% … &×（）——+【】{};；● &～|\s:'
punc =add_punc
print(punc)

fr1 = codecs.open('C:/Users/Administrator/Desktop/exp.txt', encoding='gbk')
fr2 = codecs.open('C:/Users/Administrator/Desktop/test.txt', encoding='gbk')
fw1 = codecs.open('C:/Users/Administrator/Desktop/newexp.txt', 'w', encoding='gbk')
fw2 = codecs.open('C:/Users/Administrator/Desktop/newtest.txt', 'w', encoding='gbk')

# 利用正则表达式替换为一个空格
for line in fr1:
    line = re.sub(r"[{}]+".format(punc),"",line)
    fw1.write(line+'')

for line in fr2:
    line = re.sub(r"[{}]+".format(punc),"",line)
    fw2.write(line+'')

fr1.close()
fw1.close()
fr2.close()
fw2.close()

class simhash:
    def __init__(self,content):
        self.simhash=self.simhash(content)

    def __str__(self):
        return str(self.simhash)

    def simhash(self,content):
        #seg = jieba.cut(content)
        #jieba.analyse.set_stop_words('stopword.txt')
        keyWord = jieba.analyse.extract_tags(
            '|'.join(content), topK=10, withWeight=True, allowPOS=())#在这里对jieba的tfidf.py进行了修改
        #将tags = sorted(freq.items(), key=itemgetter(1), reverse=True)修改成tags = sorted(freq.items(), key=itemgetter(1,0), reverse=True)
        #即先按照权重排序，再按照词排序
        keyList = []
        #print keyWord[0][0]
        # print(keyWord)
        for feature, weight in keyWord:
            weight = int(weight * 10)
            feature = self.string_hash(feature)
            temp = []
            for i in feature:
                if(i == '1'):
                    temp.append(weight)
                else:
                    temp.append(-weight)
            # print(temp)
            keyList.append(temp)
        list1 = np.sum(np.array(keyList), axis=0)
        #print(list1)
        if(keyList==[]): #编码读不出来
            return '00'
        simhash = ''
        for i in list1:
            if(i > 0):
                simhash = simhash + '1'
            else:
                simhash = simhash + '0'
        return simhash

    def similarity (self, other):
        a = float(self.simhash)
        b = float(other.simhash)
        if a > b : return b / a
        else: return a / b

    def string_hash(self,source):
        if source == "":
            return 0
        else:
            x = ord(source[0]) << 7
            m = 1000003
            mask = 2 ** 128 - 1
            for c in source:
                x = ((x * m) ^ ord(c)) & mask
            x ^= len(source)
            if x == -1:
                x = -2
            x = bin(x).replace('0b', '').zfill(64)[-64:]
            #print(source,x)

            return str(x)

    def hammingDis(self, com):
        t1 = '0b' + self.simhash
        t2 = '0b' + com.simhash
        n = int(t1, 2) ^ int(t2, 2)
        i = 0
        while n:
            n &= (n - 1)
            i += 1
        return i


if __name__ == '__main__':
    f1 = codecs.open(r"C:/Users/Administrator/Desktop/newexp.txt",'r','gbk')
    f2 = codecs.open(r"C:/Users/Administrator/Desktop/newtest.txt", 'r', 'gbk')
    docs1 = f1.readlines()
    docs2 = f2.readlines()
    stoplist = {}.fromkeys([ line.rstrip() for line in codecs.open(r"C:/Users/Administrator/Desktop/word_level/data/中文停用词库.txt",'r','gbk') ])
#stoplist = {}.fromkeys(u'的')
    texts1 = [[word for word in document.split() if word not in stoplist]
         for document in docs1]
    texts2 = [[word for word in document.split() if word not in stoplist]
             for document in docs2]
    for i in docs1:
        #s = 'This is a test string for testing'
        s1 = i
        hash1 = simhash(s1.split())
        print(s1)
        for j in docs2:
            s2 = j
            hash2 = simhash(s2.split())
            print(s2)
            print(hash1.hammingDis(hash2))
            if hash1.hammingDis(hash2) <= 18:
                print('文本相似')
            else:
                print('文本不相似')
